{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import openai\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ML_Projects/ML/LLM_Summarization/llm/lib/python3.11/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# Access the environment variable\n",
    "openai.api_key = os.getenv(\"api_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagelevel_pdf_process(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    page_chunks_ = []\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        page_chunks_.append(text) \n",
    "    return page_chunks_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraleve_pdf_process(pdf_path):\n",
    "    paragraph_chunks_ = []\n",
    "    page_chunks_ = pagelevel_pdf_process(pdf_path)  \n",
    "    for page_text in page_chunks_:\n",
    "        page_paragraphs = page_text.split('\\n\\n')  \n",
    "        for para in page_paragraphs:\n",
    "            if para.strip():\n",
    "                paragraph_chunks_.append(para.strip())\n",
    "    return paragraph_chunks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chunks_1 = pagelevel_pdf_process(\"/Users/mohammedjohnyshaik/Documents/ML_Projects/ML/LLM_Summarization/data/Generative_AI.pdf\")\n",
    "Chunks_2 = paraleve_pdf_process(\"/Users/mohammedjohnyshaik/Documents/ML_Projects/ML/LLM_Summarization/data/Generative_AI.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7498"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = [len(chunk) for chunk in Chunks_2]\n",
    "total_size = sum(chunk_size)\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n",
      "Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')\n",
      "Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\n",
      "Model(id='dall-e-2', created=1698798177, object='model', owned_by='system')\n",
      "Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\n",
      "Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\n",
      "Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\n",
      "Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\n",
      "Model(id='gpt-4', created=1687882411, object='model', owned_by='openai')\n",
      "Model(id='babbage-002', created=1692634615, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\n",
      "Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\n",
      "Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\n",
      "Model(id='gpt-4o', created=1715367049, object='model', owned_by='system')\n",
      "Model(id='davinci-002', created=1692634301, object='model', owned_by='system')\n",
      "Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\n"
     ]
    }
   ],
   "source": [
    "models_ = openai.models.list()\n",
    "for model in models_:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(chunks, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.replace(\"\\n\", \" \")\n",
    "        response = openai.embeddings.create(\n",
    "            input=[chunk],\n",
    "            model=model\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks = paraleve_pdf_process('/Users/mohammedjohnyshaik/Documents/ML_Projects/ML/LLM_Summarization/data/Generative_AI.pdf')\n",
    "embeddings = get_embeddings(pdf_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(embeddings):\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    embedding_array = np.array(embeddings).astype('float32')\n",
    "    index.add(embedding_array)\n",
    "    return index\n",
    "index = store_embeddings(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_tokens):\n",
    "    tokens = text.split()[:max_tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, index, chunks, temperature=0.5, top_p=1.0):\n",
    "    query_vector = get_embeddings([query])[0]\n",
    "    _, top_indices = index.search(np.array([query_vector]), k=5)\n",
    "    relevant_chunks = [chunks[i] for i in top_indices[0]]\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": context + \"\\n\\nQ: \" + query}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature= temperature,\n",
    "        top_p = top_p\n",
    "    )\n",
    "    message_content = response.choices[0].message.content.strip()\n",
    "    #message_content = response.choices[0].message[\"content\"].strip()\n",
    "    return message_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Generative AI, including DiPusion models, is a transformative technology that generates data by refining noisy input to produce high-quality output. Its applications span content creation, art, NLP, and healthcare. The ethical considerations include bias, deepfakes, IP rights, and privacy concerns. The future of generative AI involves enhanced realism, integration with emerging tech, human-AI collaboration, and a focus on ethical development. Key technologies include neural networks, GANs, transformers, and VAEs. Overall, generative AI holds immense potential but requires careful consideration of ethical implications for responsible use.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Summarize the document\"\n",
    "response = generate_response(\n",
    "    query=sample_query,\n",
    "    index=index,\n",
    "    chunks=pdf_chunks,\n",
    "    temperature=0.5,  \n",
    "    top_p=0.9       \n",
    ")\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
