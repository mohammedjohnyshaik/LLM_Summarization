{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import openai\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagelevel_pdf_process(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    page_chunks_ = []\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        page_chunks_.append(text) \n",
    "    return page_chunks_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paralevel_pdf_process(pdf_path):\n",
    "    paragraph_chunks_ = []\n",
    "    page_chunks_ = pagelevel_pdf_process(pdf_path)  \n",
    "    for page_text in page_chunks_:\n",
    "        page_paragraphs = page_text.split('\\n\\n')  \n",
    "        for para in page_paragraphs:\n",
    "            if para.strip():\n",
    "                paragraph_chunks_.append(para.strip())\n",
    "    return paragraph_chunks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chunks_1 = pagelevel_pdf_process(\"/Users/mohammedjohnyshaik/Documents/ML_Projects/ML/LLM_Summarization/data/Generative_AI.pdf\")\n",
    "Chunks_2 = paralevel_pdf_process(\"/Users/mohammedjohnyshaik/Documents/ML_Projects/ML/LLM_Summarization/data/Generative_AI.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = [len(chunk) for chunk in Chunks_2]\n",
    "total_size = sum(chunk_size)\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\n",
      "Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\n",
      "Model(id='dall-e-2', created=1698798177, object='model', owned_by='system')\n",
      "Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\n",
      "Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\n",
      "Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\n",
      "Model(id='babbage-002', created=1692634615, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n",
      "Model(id='gpt-4', created=1687882411, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\n",
      "Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\n",
      "Model(id='gpt-4o', created=1715367049, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\n",
      "Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\n",
      "Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\n",
      "Model(id='davinci-002', created=1692634301, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\n"
     ]
    }
   ],
   "source": [
    "models_ = openai.models.list()\n",
    "for model in models_:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chunk(chunk, model=\"gpt-3.5-turbo\"):\n",
    "    tokens = openai.Tokenizer.encode(chunk, model=model)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(tokenize_chunk, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    for tokenize_chunk in tokenize_chunk:\n",
    "        #chunk = chunk.replace(\"\\n\", \" \")\n",
    "        response = openai.embeddings.create(\n",
    "            input=[tokenize_chunk],\n",
    "            model=model\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks = paralevel_pdf_process('/Users/mohammedjohnyshaik/Documents/ML_Projects/ML/LLM_Summarization/data/Generative_AI.pdf')\n",
    "embeddings = get_embeddings(pdf_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(embeddings):\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    embedding_array = np.array(embeddings).astype('float32')\n",
    "    index.add(embedding_array)\n",
    "    return index\n",
    "index = store_embeddings(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''def truncate_text(text, max_tokens):\n",
    "    tokens = text.split()[:max_tokens]\n",
    "    return ' '.join(tokens)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, index, chunks, temperature=0.5, top_p=1.0):\n",
    "    query_vector = get_embeddings([query])[0]\n",
    "    _, top_indices = index.search(np.array([query_vector]), k=5)\n",
    "    relevant_chunks = [chunks[i] for i in top_indices[0]]\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": context + \"\\n\\nQ: \" + query}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature= temperature,\n",
    "        top_p = top_p\n",
    "    )\n",
    "    message_content = response.choices[0].message.content.strip()\n",
    "    return message_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Generative AI, a transformative technology, mimics human-created data and has diverse applications in content creation, art, NLP, and healthcare. Key technologies like neural networks, GANs, transformers, and VAEs drive its development. However, ethical challenges such as bias, deepfakes, IP rights, and privacy must be addressed. The future of generative AI includes enhanced realism, integration with AR/VR/IoT, human-AI collaboration, and a focus on ethical development. Overall, generative AI has vast potential to reshape industries but requires responsible use and ethical considerations.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Summarize the document\"\n",
    "response = generate_response(\n",
    "    query=sample_query,\n",
    "    index=index,\n",
    "    chunks=pdf_chunks,\n",
    "    temperature=0.5,  \n",
    "    top_p=0.9       \n",
    ")\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
